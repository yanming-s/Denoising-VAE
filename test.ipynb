{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99c967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76821e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.vit import Transformer_Layer, Patch_Embedding\n",
    "\n",
    "class ViT_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT Encoder for VAE\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_channels, latent_dim,\n",
    "                 embed_dim, depth, num_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.patch_embed = Patch_Embedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # Learnable positional embeddings for all patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "        # Stack transformer encoder layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            Transformer_Layer(embed_dim, num_heads, mlp_dim, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # VAE specific: project to latent space\n",
    "        self.ll_mu = nn.Linear(embed_dim * num_patches, latent_dim)\n",
    "        self.ll_var = nn.Linear(embed_dim * num_patches, latent_dim)\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "    def forward(self, x):\n",
    "        # x: [bs, in_channels, img_size, img_size]\n",
    "        bs = x.shape[0]\n",
    "        x = self.patch_embed(x)  # [bs, num_patches, embed_dim]\n",
    "        # Add positional embeddings and apply dropout\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        # Transformer expects shape [seq_length, batch_size, embed_dim]\n",
    "        x = x.transpose(0, 1)\n",
    "        for block in self.transformer_layers:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        # Reshape for the VAE latent space projection\n",
    "        x = x.transpose(0, 1)  # Back to [bs, num_patches, embed_dim]\n",
    "        x = x.reshape(bs, -1)  # [bs, num_patches * embed_dim]\n",
    "        # Get latent space parameters\n",
    "        mu = self.ll_mu(x)\n",
    "        log_var = self.ll_var(x)\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771dda79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in ViT Encoder: 61373312\n"
     ]
    }
   ],
   "source": [
    "encoder = ViT_Encoder(\n",
    "    img_size=256,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    latent_dim=256,\n",
    "    embed_dim=512,\n",
    "    depth=6,\n",
    "    num_heads=8,\n",
    "    mlp_dim=512*4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"Number of parameters in ViT Encoder:\", count_parameters(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6db3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block for UNet decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Upsampling followed by a convolution (transposed conv alternative)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        # Double convolution block\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Upsample\n",
    "        x = self.upsample(x)\n",
    "        # Apply double convolution\n",
    "        x = self.double_conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet Decoder for VAE\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, out_channels, latent_dim, base_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        # Calculate number of spatial dimensions needed for initial feature map\n",
    "        self.init_size = img_size // 16\n",
    "        self.latent_channels = base_channels * 8\n",
    "        # Project from latent vector to initial feature map\n",
    "        self.latent_to_features = nn.Sequential(\n",
    "            nn.Linear(latent_dim, self.init_size * self.init_size * self.latent_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        # Upsampling blocks\n",
    "        # Each block doubles the spatial dimensions and halves the channels\n",
    "        self.up1 = UpBlock(self.latent_channels, base_channels * 4, dropout)\n",
    "        self.up2 = UpBlock(base_channels * 4, base_channels * 2, dropout)\n",
    "        self.up3 = UpBlock(base_channels * 2, base_channels, dropout)\n",
    "        self.up4 = UpBlock(base_channels, base_channels, dropout)\n",
    "        # Final convolution to get the right number of output channels\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()  # Ensures output is in [0, 1] range\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        # z: [bs, latent_dim]\n",
    "        bs = z.shape[0]\n",
    "        # Project and reshape to initial feature map\n",
    "        x = self.latent_to_features(z)\n",
    "        x = x.view(bs, self.latent_channels, self.init_size, self.init_size)\n",
    "        # Upsampling path\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.up4(x)\n",
    "        # Final convolution\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc16bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in UNet Decoder: 69640899\n"
     ]
    }
   ],
   "source": [
    "decoder = UNet_Decoder(\n",
    "    img_size=256,\n",
    "    out_channels=3,\n",
    "    latent_dim=512,\n",
    "    base_channels=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"Number of parameters in UNet Decoder:\", count_parameters(decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b82499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5340",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
